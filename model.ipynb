{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic Parkinson’s Dataset Head:\n",
      "             name  MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
      "0  phon_R01_S01_1   197.810593    199.547511    196.148841        0.000448   \n",
      "1  phon_R01_S01_1   119.992000    157.302000     74.997000        0.007840   \n",
      "2  phon_R01_S01_2   122.400000    148.650000    113.819000        0.009680   \n",
      "3  phon_R01_S01_3   116.682000    131.111000    111.555000        0.010500   \n",
      "4  phon_R01_S01_4   116.676000    137.871000    111.366000        0.009970   \n",
      "\n",
      "   MDVP:Jitter(Abs)    MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  ...  \\\n",
      "0          0.028524 -472.782304  90.87889   84.092766     76.005376  ...   \n",
      "1          0.000070    0.003700   0.00554    0.011090      0.043740  ...   \n",
      "2          0.000080    0.004650   0.00696    0.013940      0.061340  ...   \n",
      "3          0.000090    0.005440   0.00781    0.016330      0.052330  ...   \n",
      "4          0.000090    0.005020   0.00698    0.015050      0.054920  ...   \n",
      "\n",
      "   Shimmer:DDA       NHR        HNR  status      RPDE        DFA   spread1  \\\n",
      "0     15.51825  5.048163  -3.520193       0 -9.955684 -14.253116  0.075602   \n",
      "1      0.06545  0.022110  21.033000       1  0.414783   0.815285 -4.813031   \n",
      "2      0.09403  0.019290  19.085000       1  0.458359   0.819521 -4.075192   \n",
      "3      0.08270  0.013090  20.651000       1  0.429895   0.825288 -4.443179   \n",
      "4      0.08771  0.013530  20.644000       1  0.434969   0.819235 -4.117501   \n",
      "\n",
      "      spread2         D2       PPE  \n",
      "0  206.114521  27.933184  0.000000  \n",
      "1    0.266482   2.301442  0.284654  \n",
      "2    0.335590   2.486855  0.368674  \n",
      "3    0.311173   2.342259  0.332634  \n",
      "4    0.334147   2.405554  0.368975  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "\n",
    "file_path_basic = r\"C:\\Users\\shail\\OneDrive\\Desktop\\dverse project\\1\\parkinsons.data\"\n",
    "\n",
    "# Try reading the files\n",
    "try:\n",
    "    \n",
    "    df_basic = pd.read_csv(file_path_basic)\n",
    "\n",
    "\n",
    "    print(\"\\nBasic Parkinson’s Dataset Head:\")\n",
    "    print(df_basic.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error loading files:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 3s 30ms/step - loss: 0.6651 - accuracy: 0.7564 - val_loss: 0.5364 - val_accuracy: 0.7500\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.4858 - accuracy: 0.7756 - val_loss: 0.5272 - val_accuracy: 0.7000\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.3678 - accuracy: 0.8462 - val_loss: 0.4989 - val_accuracy: 0.7500\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.2947 - accuracy: 0.8526 - val_loss: 0.5304 - val_accuracy: 0.8250\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.3474 - accuracy: 0.8205 - val_loss: 0.4094 - val_accuracy: 0.8250\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.2759 - accuracy: 0.8782 - val_loss: 0.4101 - val_accuracy: 0.8500\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2654 - accuracy: 0.8654 - val_loss: 0.4660 - val_accuracy: 0.8250\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2777 - accuracy: 0.8718 - val_loss: 0.3908 - val_accuracy: 0.8500\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2391 - accuracy: 0.8846 - val_loss: 0.3799 - val_accuracy: 0.8750\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1928 - accuracy: 0.9231 - val_loss: 0.3772 - val_accuracy: 0.9000\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1870 - accuracy: 0.9038 - val_loss: 0.4060 - val_accuracy: 0.8750\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1960 - accuracy: 0.8974 - val_loss: 0.3817 - val_accuracy: 0.9000\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1643 - accuracy: 0.9295 - val_loss: 0.4019 - val_accuracy: 0.9000\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.8974 - val_loss: 0.4245 - val_accuracy: 0.8750\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.9423 - val_loss: 0.4186 - val_accuracy: 0.9000\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1257 - accuracy: 0.9487 - val_loss: 0.3963 - val_accuracy: 0.9000\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1157 - accuracy: 0.9615 - val_loss: 0.4497 - val_accuracy: 0.9000\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1161 - accuracy: 0.9487 - val_loss: 0.4348 - val_accuracy: 0.8750\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1208 - accuracy: 0.9423 - val_loss: 0.4326 - val_accuracy: 0.9000\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0881 - accuracy: 0.9679 - val_loss: 0.4826 - val_accuracy: 0.9000\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0806 - accuracy: 0.9744 - val_loss: 0.4866 - val_accuracy: 0.9250\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1204 - accuracy: 0.9359 - val_loss: 0.4347 - val_accuracy: 0.9000\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9551 - val_loss: 0.5886 - val_accuracy: 0.8750\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1122 - accuracy: 0.9615 - val_loss: 0.4454 - val_accuracy: 0.9000\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1952 - accuracy: 0.9167 - val_loss: 0.5702 - val_accuracy: 0.8500\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1059 - accuracy: 0.9615 - val_loss: 0.4508 - val_accuracy: 0.8750\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9808 - val_loss: 0.4789 - val_accuracy: 0.8750\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 0.9872 - val_loss: 0.5029 - val_accuracy: 0.8750\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0583 - accuracy: 0.9808 - val_loss: 0.5510 - val_accuracy: 0.9000\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0761 - accuracy: 0.9679 - val_loss: 0.4860 - val_accuracy: 0.9000\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "XGBoost Classifier Accuracy: 0.9250\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.91      1.00      0.95        30\n",
      "\n",
      "    accuracy                           0.93        40\n",
      "   macro avg       0.95      0.85      0.89        40\n",
      "weighted avg       0.93      0.93      0.92        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\shail\\OneDrive\\Desktop\\dverse project\\1\\parkinsons.data\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the 'name' column (not needed for training)\n",
    "df.drop(columns=['name'], inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['status']).values  # Features\n",
    "y = df['status'].values  # Labels (0 = Healthy, 1 = Parkinson's)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape data for CNN (convert 1D to 2D spectrogram-like shape)\n",
    "X_reshaped = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define CNN model\n",
    "def build_cnn_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(256, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', name=\"feature_layer\"), # Feature extraction layer\n",
    "        layers.Dense(1, activation='sigmoid')  # Binary classification layer\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build and train CNN\n",
    "cnn_model = build_cnn_model(X_train.shape[1:])\n",
    "cnn_model.fit(X_train, y_train, epochs=30, batch_size=8, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Extract deep features from CNN (before classification layer)\n",
    "feature_extractor = keras.Model(inputs=cnn_model.input, outputs=cnn_model.get_layer(\"feature_layer\").output)\n",
    "X_train_features = feature_extractor.predict(X_train)\n",
    "X_test_features = feature_extractor.predict(X_test)\n",
    "\n",
    "\n",
    "# Train XGBoost classifier on extracted features\n",
    "xgb_model = XGBClassifier(eval_metric='logloss')\n",
    "xgb_model.fit(X_train_features, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = xgb_model.predict(X_test_features)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoost Classifier Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_model.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming scaler, feature_extractor, and xgb_model are trained\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(feature_extractor, \"feature_extractor.pkl\")\n",
    "joblib.dump(xgb_model, \"xgb_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "feature_extractor = joblib.load(\"feature_extractor.pkl\")\n",
    "xgb_model = joblib.load(\"xgb_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "Prediction: Healthy\n"
     ]
    }
   ],
   "source": [
    "# Function to make a new prediction\n",
    "def predict_parkinsons(new_data):\n",
    "    # Convert input to numpy array and reshape\n",
    "    new_data = np.array(new_data).reshape(1, -1)\n",
    "    \n",
    "    # Normalize using the same scaler\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "    \n",
    "    # Reshape to match CNN input shape\n",
    "    new_data_reshaped = new_data_scaled.reshape(1, new_data_scaled.shape[1], 1)\n",
    "    \n",
    "    # Extract deep features using CNN\n",
    "    new_features = feature_extractor.predict(new_data_reshaped)\n",
    "    \n",
    "    # Predict with XGBoost\n",
    "    prediction = xgb_model.predict(new_features)\n",
    "    \n",
    "    # Output result\n",
    "    return \"Parkinson’s Detected\" if prediction[0] == 1 else \"Healthy\"\n",
    "\n",
    "# Example input (replace with actual values from new patient)\n",
    "sample_input = [241.40400,248.83400,232.48300,0.00281,0.00001,0.00157,0.00173,0.00470,0.01760,0.15400,0.01006,0.01038,0.01251,0.03017,0.00675,23.14500,0.457702,0.634267,-6.793547,0.158266,2.256699,0.117399\n",
    "]\n",
    "# Add one more value!\n",
    " # Example from dataset\n",
    "\n",
    "# Make prediction\n",
    "result = predict_parkinsons(sample_input)\n",
    "print(\"Prediction:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.90\n",
      "Random Forest Accuracy: 0.93\n",
      "XGBoost Accuracy: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shail\\OneDrive\\Desktop\\dverse project\\.venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [05:50:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# Train SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train_features, y_train)\n",
    "y_pred_svm = svm.predict(X_test_features)\n",
    "svm_acc = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train_features, y_train)\n",
    "y_pred_rf = rf.predict(X_test_features)\n",
    "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Train XGBoost\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_features, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test_features)\n",
    "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Print accuracy scores\n",
    "print(f\"SVM Accuracy: {svm_acc:.2f}\")\n",
    "print(f\"Random Forest Accuracy: {rf_acc:.2f}\")\n",
    "print(f\"XGBoost Accuracy: {xgb_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted Features [197.8105931050871, 199.54751131221718, 196.14884107493458, 0.0004481281172223465, 0.028523533, -472.7823043973156, 90.8788900213032, 84.09276620887884, 76.00537613235005, 65.34663081368411, 53.21114203479015, 40.282371978794444, 27.477788229635625, 15.518249630446077, 5.048162963071039, -3.52019325415012, -9.955683710503264, -14.253115923648428, 0.07560193101427107, 206.11452087079246, 27.933184302255373, 0.0]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Prediction: Healthy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.signal import butter, filtfilt\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import ShortTermFeatures\n",
    "\n",
    "\n",
    "def extract_audio_features(file_path):\n",
    "    \"\"\"Extract 22 audio features and align them with Parkinson's dataset format.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=44100)  # Ensure correct sampling rate\n",
    "\n",
    "        # Apply a strong low-pass filter to remove unwanted high frequencies\n",
    "        def apply_lowpass_filter(y, sr, cutoff=500):\n",
    "            nyquist = 0.5 * sr\n",
    "            normal_cutoff = cutoff / nyquist\n",
    "            b, a = butter(6, normal_cutoff, btype='low', analog=False)\n",
    "            return filtfilt(b, a, y)\n",
    "\n",
    "        y = apply_lowpass_filter(y, sr, cutoff=500)  # Ensure no frequencies above 500Hz\n",
    "\n",
    "        features = []\n",
    "\n",
    "        # Extract MDVP:Fo (Fundamental Frequency)\n",
    "        features.append(np.mean(librosa.yin(y, fmin=190, fmax=210, sr=sr)))  \n",
    "\n",
    "        # Extract MDVP:Fhi & MDVP:Flo (Using pitch estimation instead of spectral methods)\n",
    "        features.append(np.max(librosa.yin(y, fmin=200, fmax=220, sr=sr)))  # Max pitch\n",
    "        features.append(np.min(librosa.yin(y, fmin=180, fmax=200, sr=sr)))  # Min pitch\n",
    "\n",
    "        # Extract jitter & shimmer values\n",
    "        features.append(np.std(librosa.feature.zero_crossing_rate(y=y)))  # MDVP:Jitter(%)\n",
    "        features.append(np.std(librosa.feature.rms(y=y)))  # MDVP:Jitter(Abs)\n",
    "\n",
    "\n",
    "        # 13 MFCCs mapped to jitter, shimmer, RPDE, DFA, and spread values\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc_means = np.mean(mfccs, axis=1)\n",
    "        features.extend(mfcc_means.tolist())\n",
    "\n",
    "        # Extract additional features using PyAudioAnalysis\n",
    "        [fs, x] = audioBasicIO.read_audio_file(file_path)\n",
    "        x = audioBasicIO.stereo_to_mono(x)\n",
    "        short_features, _ = ShortTermFeatures.feature_extraction(x, fs, 0.050 * fs, 0.025 * fs)\n",
    "\n",
    "        # Energy Entropy → RPDE\n",
    "        energy_entropy = np.mean(short_features[1]) if len(short_features) > 1 else 0\n",
    "        features.append(energy_entropy)\n",
    "\n",
    "        # Fundamental Frequency (Pitch) → D2\n",
    "        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "        pitch_mean = np.mean(pitches[pitches > 0]) if pitches.any() else 0\n",
    "        features.append(pitch_mean)\n",
    "\n",
    "        # Harmonic-to-Noise Ratio (HNR) → HNR\n",
    "        harmonic, percussive = librosa.effects.hpss(y)\n",
    "        hnr = np.sum(np.abs(harmonic)) / (np.sum(np.abs(percussive)) + 1e-6)\n",
    "        features.append(hnr)\n",
    "\n",
    "        # Tempo → PPE\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        features.append(float(tempo))\n",
    "\n",
    "        # Ensure correct feature count\n",
    "        if len(features) != 22:\n",
    "            print(f\"Feature extraction issue! Expected 22, got {len(features)}\")\n",
    "            return None\n",
    "        \n",
    "        print(\"extracted Features\",features)\n",
    "\n",
    "        return np.array(features)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to make a new prediction\n",
    "def predict_parkinsons(new_data):\n",
    "    new_data = np.array(new_data).reshape(1, -1)\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "    new_data_reshaped = new_data_scaled.reshape(1, new_data_scaled.shape[1], 1)\n",
    "\n",
    "    # Extract deep features using CNN\n",
    "    new_features = feature_extractor.predict(new_data_reshaped)\n",
    "\n",
    "    # Predict with XGBoost\n",
    "    prediction = xgb_model.predict(new_features)\n",
    "\n",
    "    return \"Parkinson’s Detected\" if prediction[0] == 1 else \"Healthy\"\n",
    "\n",
    "# Example Usage\n",
    "file_path = r\"C:\\Users\\shail\\Downloads\\final_fine_tuned_audio.wav\"\n",
    "extracted_features = extract_audio_features(file_path)\n",
    "\n",
    "if extracted_features is not None:\n",
    "    result = predict_parkinsons(extracted_features)\n",
    "    print(\"Prediction:\", result)\n",
    "else:\n",
    "    print(\"Feature extraction failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing C:\\Users\\shail\\Downloads\\final_fine_tuned_audio.wav: name 'audioBasicIO' is not defined\n",
      "Feature extraction failed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def extract_audio_features(file_path):\n",
    "    \"\"\"Extract 22 audio features and align them with Parkinson's dataset format.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        features = []\n",
    "\n",
    "        # Extracting features based on Parkinson’s dataset order\n",
    "        features.append(np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)))  # MDVP:Fo(Hz)\n",
    "        features.append(np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr)))  # MDVP:Fhi(Hz)\n",
    "        features.append(np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)))  # MDVP:Flo(Hz)\n",
    "        features.append(np.mean(librosa.feature.zero_crossing_rate(y=y)))  # MDVP:Jitter(%)\n",
    "        features.append(np.mean(librosa.feature.rms(y=y)))  # MDVP:Jitter(Abs)\n",
    "\n",
    "        # 13 MFCCs mapped to jitter, shimmer, RPDE, DFA, and spread values\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc_means = np.mean(mfccs, axis=1)\n",
    "        features.extend(mfcc_means.tolist())\n",
    "\n",
    "        # Extract additional features using PyAudioAnalysis\n",
    "        [fs, x] = audioBasicIO.read_audio_file(file_path)\n",
    "        x = audioBasicIO.stereo_to_mono(x)\n",
    "        short_features, _ = ShortTermFeatures.feature_extraction(x, fs, 0.050 * fs, 0.025 * fs)\n",
    "\n",
    "        # Energy Entropy → RPDE\n",
    "        energy_entropy = np.mean(short_features[1]) if len(short_features) > 1 else 0\n",
    "        features.append(energy_entropy)\n",
    "\n",
    "        # Fundamental Frequency (Pitch) → D2\n",
    "        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "        pitch_mean = np.mean(pitches[pitches > 0]) if pitches.any() else 0\n",
    "        features.append(pitch_mean)\n",
    "\n",
    "        # Harmonic-to-Noise Ratio (HNR) → HNR\n",
    "        harmonic, percussive = librosa.effects.hpss(y)\n",
    "        hnr = np.sum(np.abs(harmonic)) / (np.sum(np.abs(percussive)) + 1e-6)\n",
    "        features.append(hnr)\n",
    "\n",
    "        # Tempo → PPE\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        features.append(float(tempo))\n",
    "\n",
    "        # Ensure correct feature count\n",
    "        if len(features) != 22:\n",
    "            print(f\"Feature extraction issue! Expected 22, got {len(features)}\")\n",
    "            return None\n",
    "        \n",
    "        print(\"extracted Features\",features)\n",
    "\n",
    "        return np.array(features)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "# Function to make a new prediction\n",
    "def predict_parkinsons(new_data):\n",
    "    new_data = np.array(new_data).reshape(1, -1)\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "    new_data_reshaped = new_data_scaled.reshape(1, new_data_scaled.shape[1], 1)\n",
    "\n",
    "    # Extract deep features using CNN\n",
    "    new_features = feature_extractor.predict(new_data_reshaped)\n",
    "\n",
    "    # Predict with XGBoost\n",
    "    prediction = xgb_model.predict(new_features)\n",
    "\n",
    "    return \"Parkinson’s Detected\" if prediction[0] == 1 else \"Healthy\"\n",
    "\n",
    "# Example Usage\n",
    "file_path = r\"C:\\Users\\shail\\Downloads\\final_fine_tuned_audio.wav\"\n",
    "extracted_features = extract_audio_features(file_path)\n",
    "\n",
    "if extracted_features is not None:\n",
    "    result = predict_parkinsons(extracted_features)\n",
    "    print(\"Prediction:\", result)\n",
    "else:\n",
    "    print(\"Feature extraction failed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
